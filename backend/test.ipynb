{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kavitha\\StudioProjects\\bruno_app\\backend\\bruno_server_venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from pinecone import ServerlessSpec\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_apikey=os.getenv(\"PINECONE_API_KEY\")\n",
    "groq_apikey=os.getenv(\"GROQ_API_KEY\")\n",
    "pinecone_environment=os.getenv(\"PINECONE_ENVIRONMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kavitha\\AppData\\Local\\Temp\\ipykernel_3564\\3655315981.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "C:\\Users\\Kavitha\\AppData\\Local\\Temp\\ipykernel_3564\\3655315981.py:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "pc = Pinecone(pinecone_apikey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cloud ='aws'\n",
    "region =pinecone_environment\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_vectorize(pc,spec,embeddings):\n",
    "    UPLOAD_FOLDER = Path(\"uploaded_files\")\n",
    "    all_text = \"\"\n",
    "\n",
    "    for file_path in UPLOAD_FOLDER.glob(\"*\"):\n",
    "        if file_path.suffix == \".pdf\":\n",
    "            loader = PyPDFLoader(str(file_path))\n",
    "        elif file_path.suffix in [\".doc\", \".docx\"]:\n",
    "            loader = Docx2txtLoader(str(file_path))\n",
    "        elif file_path.suffix == \".txt\":\n",
    "            loader = TextLoader(str(file_path))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        documents = loader.load()\n",
    "        all_text += \" \".join([doc.page_content for doc in documents])\n",
    "\n",
    "    # Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(all_text)\n",
    "\n",
    "    index_name = \"bruno\"\n",
    "\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(index_name, dimension=768, metric=\"cosine\",spec=spec)\n",
    "    else:\n",
    "        print(\"already there\")\n",
    "\n",
    "    index = pc.Index(index_name)\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    vector_store = Pinecone.from_texts(chunks, embeddings, index_name=index_name)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already there\n"
     ]
    }
   ],
   "source": [
    "vector_store = extract_and_vectorize(pc, spec, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to slide 2, the problem statement is \"Instant Video Creation Through Text\". This means taking a PDF brochure as input and converting it to a video, making a quiz, and showing analytics for both the video and quiz.According to slide 2, the problem statement is \"Instant Video Creation Through Text\". This means taking a PDF brochure as input and converting it to a video, making a quiz, and showing analytics for both the video and quiz.\n"
     ]
    }
   ],
   "source": [
    "for txt in qa.stream({\"query\": \"What is our problem statement?\"}):\n",
    "    print(txt['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Truncate\n",
    "index_name=\"bruno\"\n",
    "index=pc.Index(index_name)\n",
    "index.delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "vector_store1 = Pinecone.from_existing_index(index_name=\"bruno\", embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=groq_apikey,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    streaming=True,\n",
    "   callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store1.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bruno_server_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
